{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 16\n",
    "\n",
    "Name:  \n",
    "UID: \n",
    "\n",
    "### Topics\n",
    "\n",
    "- Gradient Descent\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Recall in Linear Regression we are trying to find the line $$y = X \\beta$$ that minimizes the sum of square distances between the predicted `y` and the `y` we observed in our dataset:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{\\beta}) = \\Vert \\mathbf{y} - X\\mathbf{\\beta} \\Vert^2$$\n",
    "\n",
    "We were able to find a global minimum to this loss function but we will try to apply gradient descent to find that same solution.\n",
    "\n",
    "a) Implement the `loss` function to complete the code and plot the loss as a function of beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "beta = np.array([ 1 , .5 ])\n",
    "xlin = -10.0 + 20.0 * np.random.random(100)\n",
    "X = np.column_stack([np.ones((len(xlin), 1)), xlin])\n",
    "y = beta[0]+(beta[1]*xlin)+np.random.randn(100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xlin, y,'ro',markersize=4)\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(min(y), max(y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = np.arange(-5, 4, 0.1)\n",
    "b1 = np.arange(-5, 4, 0.1)\n",
    "b0, b1 = np.meshgrid(b0, b1)\n",
    "\n",
    "def loss(X, y, beta):\n",
    "    return np.linalg.norm(y-X @ beta)**2\n",
    "    #return np.sum((y-X @ beta)**2)\n",
    "\n",
    "def get_cost(B0, B1):\n",
    "    res = []\n",
    "    for b0, b1 in zip(B0, B1):\n",
    "        line = []\n",
    "        for i in range(len(b0)):\n",
    "            beta = np.array([b0[i], b1[i]])\n",
    "            line.append(loss(X, y, beta))\n",
    "        res.append(line)\n",
    "    return np.array(res)\n",
    "\n",
    "cost = get_cost(b0, b1)\n",
    " \n",
    "# Creating figure\n",
    "fig = plt.figure(figsize =(4, 4))\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.set_xlim(-6, 4)\n",
    "ax.set_xlabel(r'$\\beta_0$')\n",
    "ax.set_ylabel(r'$\\beta_1$')\n",
    "ax.set_ylim(-5, 4)\n",
    "ax.set_zlim(0, 30000)\n",
    "\n",
    "# Creating plot\n",
    "ax.plot_surface(b0, b1, cost, alpha=.7)\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the loss is\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{\\beta}) = \\Vert \\mathbf{y} - X\\mathbf{\\beta} \\Vert^2 = \\beta^T X^T X \\beta - 2\\mathbf{\\beta}^TX^T\\mathbf{y}  + \\mathbf{y}^T\\mathbf{y}$$\n",
    "\n",
    "The gradient is\n",
    "\n",
    "$$\\nabla_\\beta \\mathcal{L}(\\mathbf{\\beta}) = 2X^T X \\beta - 2X^T\\mathbf{y}$$\n",
    "\n",
    "b) Implement the gradient function below and complete the gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "\n",
    "def snap(betas, losses):\n",
    "    # Creating figure\n",
    "    fig = plt.figure(figsize =(14, 9))\n",
    "    ax = plt.axes(projection ='3d')\n",
    "    ax.view_init(20, -20)\n",
    "    ax.set_xlim(-5, 4)\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "    ax.set_ylim(-5, 4)\n",
    "    ax.set_zlim(0, 30000)\n",
    "\n",
    "    # Creating plot\n",
    "    ax.plot_surface(b0, b1, cost, color='b', alpha=.7)\n",
    "    ax.plot(np.array(betas)[:,0], np.array(betas)[:,1], losses, 'o-', c='r', markersize=10, zorder=10)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "\n",
    "def gradient(X, y, beta):\n",
    "    return 2 * X.T @ ( X @ beta-y)\n",
    "  \n",
    "\n",
    "\n",
    "def gradient_descent(X, y, beta_hat, learning_rate, epochs, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        images.append(snap(betas, losses))\n",
    "        beta_hat = beta_hat - learning_rate * gradient(X, y, beta_hat)\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses)\n",
    "\n",
    "\n",
    "beta_start = np.array([-5, -2])\n",
    "learning_rate = 0.0005 # try .0005\n",
    "images = []\n",
    "betas, losses = gradient_descent(X, y, beta_start, learning_rate, 10, images)\n",
    "\n",
    "images[0].save(\n",
    "    'gd.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use the code above to create an animation of the linear model learned at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snap_model(beta):\n",
    "    xplot = np.linspace(-10,10,50)\n",
    "    yestplot = beta[0]+(beta[1]*xplot)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(xplot, yestplot,'b-',lw=2)\n",
    "    ax.plot(xlin, y,'ro',markersize=4)\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(min(y), max(y))\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, beta_hat, learning_rate, epochs, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        images.append(snap_model(beta_hat))\n",
    "        beta_hat = beta_hat - learning_rate * gradient(X, y, beta_hat)\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses)\n",
    "\n",
    "\n",
    "images = []\n",
    "betas, losses = gradient_descent(X, y, beta_start, learning_rate, 100, images)\n",
    "\n",
    "images[0].save(\n",
    "    'model.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, the `loss` is the negative log-likelihood\n",
    "\n",
    "$$ \\mathcal{l}(\\mathbf{\\beta}) = - \\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\sigma(x_i \\beta)) + (1 - y_i)\\log(1 - \\sigma(x_i \\beta))$$\n",
    "\n",
    "the gradient of which is:\n",
    "\n",
    "$$\\nabla_\\beta \\mathcal{l}(\\mathbf{\\beta}) = \\frac{1}{N} \\sum_{i=1}^{N} x_i (y_i - \\sigma(x_i \\beta)) $$\n",
    "\n",
    "d) Plot the loss as a function of b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "centers = [[0, 0]]\n",
    "t, _ = datasets.make_blobs(n_samples=100, centers=centers, cluster_std=2, random_state=0)\n",
    "\n",
    "# LINE\n",
    "def generate_line_data():\n",
    "    # create some space between the classes\n",
    "    X = t\n",
    "    Y = np.array([1 if x[0] - x[1] >= 0 else 0 for x in X])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, y = generate_line_data()\n",
    "\n",
    "cs = np.array([x for x in 'gb'])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], color=cs[y].tolist(), s=50, alpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = np.arange(-20, 20, 0.1)\n",
    "b1 = np.arange(-20, 20, 0.1)\n",
    "b0, b1 = np.meshgrid(b0, b1)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    e = np.exp(x)\n",
    "    return e / (1 + e)\n",
    "\n",
    "\n",
    "def loss(X, Y, beta):\n",
    "    #return np.sum((y - sigmoid(X @ beta))**2)\n",
    "    nlh = 0\n",
    "    for x,y in zip(X,Y):\n",
    "        if Y==1:\n",
    "            nlh += np.log(sigmoid(x @ beta))\n",
    "        else:\n",
    "            nlh += np.log(1-sigmoid(x @ beta))\n",
    "       \n",
    "    return -nlh/len(X)\n",
    "\n",
    "\n",
    "\n",
    "def get_cost(B0, B1):\n",
    "    res = []\n",
    "    for b0, b1 in zip(B0, B1):\n",
    "        line = []\n",
    "        for i in range(len(b0)):\n",
    "            beta = np.array([b0[i], b1[i]])\n",
    "            line.append(loss(X, y, beta))\n",
    "        res.append(line)\n",
    "    return np.array(res)\n",
    "\n",
    "cost = get_cost(b0, b1)\n",
    "\n",
    "# Creating figure\n",
    "fig = plt.figure(figsize =(14, 9))\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.set_xlim(-20, 20)\n",
    "ax.set_xlabel(r'$\\beta_0$')\n",
    "ax.set_ylabel(r'$\\beta_1$')\n",
    "ax.set_ylim(-20, 20)\n",
    "ax.set_zlim(0, 10)\n",
    "\n",
    "# Creating plot\n",
    "ax.plot_surface(b0, b1, cost, alpha=.7)\n",
    " \n",
    "# show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Plot the loss at each iteration of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "\n",
    "def snap(betas, losses):\n",
    "    # Creating figure\n",
    "    fig = plt.figure(figsize =(14, 9))\n",
    "    ax = plt.axes(projection ='3d')\n",
    "    ax.view_init(10, 10)\n",
    "    ax.set_xlabel(r'$\\beta_0$')\n",
    "    ax.set_ylabel(r'$\\beta_1$')\n",
    "    ax.set_ylim(-20, 20)\n",
    "    ax.set_zlim(0, 10)\n",
    "\n",
    "    # Creating plot\n",
    "    ax.plot_surface(b0, b1, cost, color='b', alpha=.7)\n",
    "    ax.plot(np.array(betas)[:,0], np.array(betas)[:,1], losses, 'o-', c='r', markersize=10, zorder=10)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "\n",
    "def gradient(X, y, beta):\n",
    "    return np.dot(X.T, y-sigmoid(X @ beta) ) \n",
    "    \n",
    "def gradient_descent(X, y, beta_hat, learning_rate, epochs, images):\n",
    "    losses = [loss(X, y, beta_hat)]\n",
    "    betas = [beta_hat]\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        images.append(snap(betas, losses))\n",
    "        beta_hat = beta_hat - learning_rate * gradient(X, y, beta_hat)\n",
    "\n",
    "        losses.append(loss(X, y, beta_hat))\n",
    "        betas.append(beta_hat)\n",
    "        \n",
    "    return np.array(betas), np.array(losses)\n",
    "\n",
    "\n",
    "beta_start = np.array([-5, -2])\n",
    "learning_rate = 0.1\n",
    "images = []\n",
    "betas, losses = gradient_descent(X, y, beta_start, learning_rate, 10, images)\n",
    "\n",
    "images[0].save(\n",
    "    'gd_logit.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Create an animation of the logistic regression fit at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "\n",
    "def snap_model(beta):\n",
    "    # Creating figure\n",
    "    fig = plt.figure(figsize =(14, 9))\n",
    "    ax = plt.axes()\n",
    "    ax.set_xlabel(r'$x_0$')\n",
    "    ax.set_ylabel(r'$x_1$')\n",
    "    ax.set_ylim(-20, 20)\n",
    "    ax.set_xlim(-20, 20)\n",
    "\n",
    "    # Creating plot\n",
    "    ax.scatter(X[:, 0], X[:, 1], color=cs[y].tolist(), s=50, alpha=0.9)\n",
    "    x = np.arange(-20, 20, 0.1)\n",
    "    y = -(beta[0] + beta[1] * x) / beta[2]\n",
    "    ax.plot(x, y, 'r-')\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "snap_model(betas[0]).save( 'gd_logit_model.gif', optimize=False, save_all=True, append_images=[snap_model(b) for b in betas[1:]], loop=0, duration=500)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
